# -*- coding: utf-8 -*-
"""ML_Lab6ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p1LEOn_mjy-RtIFpeej4-3hT9RT6SDfH

<h1>Ques1
"""

import pandas as pd

data = pd.read_csv('./sms.tsv', sep='\t', names=["labels", "message"])
data.head()

data.describe()

data['length'] = data['message'].apply(len)
data.head()

import string
message = 'the sample message!...'
no_p = [char for char in message if char not in string.punctuation]
no_p=''.join(no_p)
print(no_p)

from nltk.corpus import stopwords
stopwords.words('english')[0:10]

clean_message = [word for word in no_p.split() if word.lower() not in stopwords.words('english')]
print(clean_message)

def text_process(message):
    no_p =[char for char in message if char not in string.punctuation]
    no_p=''.join(no_p)
    return [word for word in no_p.split() if word.lower() not in stopwords.words('english')]

data['message'].head(5).apply(text_process)

from sklearn.feature_extraction.text import CountVectorizer
bow_transformer = CountVectorizer(analyzer=text_process).fit(data['message'])
print(len(bow_transformer.vocabulary_))

message10 = data['message'][10]
print(message10)
bow10 = bow_transformer.transform([message10])
print(bow10)
print(bow10.shape)

data_bow = bow_transformer.transform(data['message'])

print('shape of matrix: ', data_bow.shape)
print('Amount of non-zero occurences:',data_bow.nnz)

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer=TfidfTransformer().fit(data_bow)

data_tfidf=tfidf_transformer.transform(data_bow)
print(data_tfidf.shape)

from sklearn.naive_bayes import MultinomialNB
spam_detect_model = MultinomialNB().fit(data_tfidf,data['labels'])

all_predictions = spam_detect_model.predict(data_tfidf)
print(all_predictions)

from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(data['labels'],all_predictions))
print(confusion_matrix(data['labels'],all_predictions))

from sklearn.model_selection import train_test_split
msg_train,msg_test,label_train,label_test = train_test_split(data['message'],data['labels'],test_size=0.2)
print(len(msg_train),len(msg_test),len(label_train),len(label_test))

from sklearn.pipeline import Pipeline
pipeline = Pipeline([
   ( 'bow',CountVectorizer(analyzer=text_process)),
    ('tfidf',TfidfTransformer()),
    ('classifier',MultinomialNB()),
])

pipeline.fit(msg_train,label_train)

predictions = pipeline.predict(msg_test)

print(classification_report(predictions,label_test))

"""<h1>Ques2"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import LabelEncoder

from sklearn import datasets
iris = pd.read_csv("./iris_csv.csv")
iris.head()

le = LabelEncoder()
le.fit(iris['class'])
iris['class'] = le.transform(iris['class'])

iris_x = iris.iloc[:,[0, 1,2 , 3]]
iris_y = iris['class']

trainSet_x, testSet_x, trainSet_y, testSet_y = train_test_split(iris_x, iris_y, test_size = 0.33)

classifier = GaussianNB()
classifier.fit(trainSet_x, trainSet_y)

predictedValues = classifier.predict(testSet_x)
print(classifier.score(testSet_x, testSet_y))

"""<h3>step by step Q2</h3>"""

import numpy as np
class GaussianNaiveBayes:
    
    def fit(self, X, y, epsilon = 1e-10):
        self.y_classes, y_counts = np.unique(y, return_counts=True)
        self.x_classes = np.array([np.unique(x) for x in X.T])
        self.phi_y = 1.0 * y_counts/y_counts.sum()
        self.u = np.array([X[y==k].mean(axis=0) for k in self.y_classes])
        self.var_x = np.array([X[y==k].var(axis=0)  + epsilon for k in self.y_classes])
        return self
    
    def predict(self, X):
        return np.apply_along_axis(lambda x: self.compute_probs(x), 1, X)
    
    def compute_probs(self, x):
        probs = np.array([self.compute_prob(x, y) for y in range(len(self.y_classes))])
        return self.y_classes[np.argmax(probs)]
    
    def compute_prob(self, x, y):
        c = 1.0 /np.sqrt(2.0 * np.pi * (self.var_x[y]))
        return np.prod(c * np.exp(-1.0 * np.square(x - self.u[y]) / (2.0 * self.var_x[y])))
    
    def evaluate(self, X, y):
        return (self.predict(X) == y).mean()

model = GaussianNaiveBayes().fit(trainSet_x, trainSet_y)
print('accuracy:',model.evaluate(testSet_x, testSet_y))

"""<h1>Q3"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

knn = KNeighborsClassifier()
k_range = list(range(1, 31))
param_grid = dict(n_neighbors=k_range)
grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=False,verbose=1)
grid_search=grid.fit(trainSet_x, trainSet_y)
print(grid_search.best_params_)

from sklearn.metrics import accuracy_score
knn = KNeighborsClassifier(n_neighbors=27)
knn.fit(trainSet_x, trainSet_y)

y_pred_knn=knn.predict(testSet_x) 
print("Accuracy: ",accuracy_score(testSet_y,y_pred_knn))